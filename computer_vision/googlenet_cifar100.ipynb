{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzs3oBFqHn6pwaUKE6auhH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronald-hk-chung/ssnotebook/blob/main/computer_vision/googlenet_cifar100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building GoogleNet (Inception V1) to train on Cifar100\n",
        "\n",
        "**Mission Statement**\n",
        "\n",
        "1. Building a GoogleNet(Inception V1) from scratch\n",
        "2. Train GoogleNet on Cifar-100 Dataset\n",
        "3. Understand variations of Inception Model from V1 V3"
      ],
      "metadata": {
        "id": "RzPDmLK6IIBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to GoogleNet (Inception V1)\n",
        "\n",
        "GoogleNet, released in 2014, was proposed by resaerch at Google (with the collaboration of various universities) in the research paper titled [Going Deeper with Convolution](https://arxiv.org/pdf/1409.4842). It set a new benchmark in object classification and detection through it sinnovative approach in ImageNet Large Scale Visual Recognition Challenge (ILSVRC) and achieve a top-5 error rate of 6.7%)\n",
        "\n",
        "GoogleNet model is particularly known for its use of Inception modules, which serve as its building blocks by using parallel convolutions wiht various filter sizes (1x1, 3x3 and 5x5) with in a single layer. The outputs from these filters are then concatenated - the fusion of outputs from various filters creates a richer representation.\n",
        "\n",
        "While the architecture is relatively deep with 22 layers, the model maintains computational efficiency despite the increase in the number of layers.\n",
        "\n",
        "Below list the key features of GoogleNet:\n",
        "\n",
        "*   Inception Module with The 1x1 Convolution\n",
        "*   Global Average Pooling\n",
        "*   Auxilliary Classifier for Training\n",
        "\n",
        "Below shows the architecture of GoogleNet\n",
        "\n",
        "<img src=\"https://github.com/ronald-hk-chung/ssnotebook/blob/main/computer_vision/assets/googlenet_architecture.png?raw=true\">"
      ],
      "metadata": {
        "id": "iwDKLtopIi0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inception Module\n",
        "\n",
        "The Inception Module is the building block of GoogleNet, as the entire model is made by stacking Inception Modules. Key features include:\n",
        "\n",
        "*   **Multi-Level Feature Extraction**:\n",
        "\n",
        "    Image object can have large variation in size. Because of such variation in images, choosing the right kernel size for performing convolution operation becomes very difficult. A larger kernel is needed to extract information of object that is distributed more in teh picture while a smaller kernel is preferred to extract information of image that is distributed less in the picture. The major approach to increase the performace of neural networks is by increasing its size and depth with the downside of overfitting and increased computational resources. GoogleNet comes up with a novel solution to form a *wider* networker rather than *depper* which is called as Inception module. Inception module consists of multiple pooling and convolution operations with different kernel sizes in parallel, instead of using just one filter of a single size. Below shows teh naive version of the Inception module.\n",
        "\n",
        "    <img src=\"https://github.com/ronald-hk-chung/ssnotebook/blob/main/computer_vision/assets/inception_module_naive.png?raw=true\">\n",
        "\n",
        "    The 'naive' incpetion module performs convolutions on input from previous layer, with 3 different size of kernels or filters specifically 1x1, 3x3 and 5x5. Max pooling is then performed with outputs then concatenated and sent to the next inception module.\n",
        "\n",
        "*   **Dimension Reduction**:\n",
        "\n",
        "    Stacking multiple layers of the `naive` inception module can increased computations significantly. To overcome this, the researchers incorporate 1x1 convolution before feeding the data into 3x3 or 5x5 convolutions.\n",
        "\n",
        "    <img src=\"https://github.com/ronald-hk-chung/ssnotebook/blob/main/computer_vision/assets/inception_module_reduction.png?raw=true\">\n",
        "\n",
        "    By using 1x1 convolution, teh module reduces dimensionlaity before applying the more expensive 3x3 and 5x5 convolutions and pooling operations.\n",
        "\n",
        "    it also results in better representation by incorporating filters of varying sizes and more layers which the network will be able to capture wider range of features in the input data\n",
        "\n",
        "    The 1x1 convsolution is also called network in teh network as it act as a mcro-neural netowkr that learns to abstract the data before the main convolution filters are applied.\n",
        "\n"
      ],
      "metadata": {
        "id": "jtTXQHkmNeL0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqbcBuTcHxix"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Any, Callable, List, Optional, Tuple\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, **kwargs: Any) -> None:\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return F.relu(x, inplace=True)\n",
        "\n",
        "class Inception(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels: int,\n",
        "                 ch1x1: int,\n",
        "                 ch3x3red: int,\n",
        "                 ch3x3: int,\n",
        "                 ch5x5red: int,\n",
        "                 ch5x5: int,\n",
        "                 pool_proj: int,\n",
        "                 conv_block: Optional[nn.Module] = None):\n",
        "        super().__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d    #Conv2d -> BatchNorm2d -> ReLU\n",
        "\n",
        "        self.branch1 = conv_block(in_channels, ch1x1, kernel_size=1)\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            conv_block(in_channels, ch3x3red, kernel_size=1),\n",
        "            conv_block(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            conv_block(in_channels, ch5x5red, kernel_size=1),\n",
        "            # conv_block(ch5x5red, kernel_size=3, padding=1)\n",
        "            conv_block(ch5x5red, kernel_size=5, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),\n",
        "            conv_block(in_channels, pool_proj, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def _forward(self, x):\n",
        "        branch1 = self.branch1(x)\n",
        "        branch2 = self.branch2(x)\n",
        "        branch3 = self.branch3(x)\n",
        "        branch4 = self.branch4(x)\n",
        "\n",
        "        return [branch1, branch2, branch3, branch4]\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "56Oq_X8DQXkJ"
      }
    }
  ]
}